{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_factory import create_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "from utils import preprocess_context\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda:1' if not torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1008073/2645792780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roberta-ru'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bce'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoints/roberta_bs8_adam_gradclip_rocauctrack_bceloss_addfeat/epoch=2-step=942.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_type = 'roberta-ru'\n",
    "\n",
    "model = create_model(model_type, 'bce', 13)\n",
    "\n",
    "model_path = 'checkpoints/roberta_bs8_adam_gradclip_rocauctrack_bceloss_addfeat/epoch=2-step=942.ckpt'\n",
    "\n",
    "def read_model(model, path):\n",
    "    state_dict = torch.load(path)['state_dict']\n",
    "    re_state_dict = {key.replace('model.', '', 1): value for key, value in state_dict.items()}\n",
    "    model.load_state_dict(re_state_dict)\n",
    "    return model\n",
    "\n",
    "new_model = read_model(model, model_path)\n",
    "\n",
    "csv_data = pd.read_csv('add_features_train.tsv', sep='\\t')\n",
    "csv_data.drop_duplicates(['context', 'phrase'], inplace=True)\n",
    "csv_data['context'] = csv_data['context'].apply(lambda x: eval(x))\n",
    "feature_cols = csv_data.columns[3:]\n",
    "\n",
    "csv_data['prep_context'] = csv_data['context'].apply(lambda x: preprocess_context(x, 'roberta'))\n",
    "csv_data['prep_phrase_context'] = csv_data['prep_context'] + 'â€” ' + csv_data['phrase'] + '</s>'\n",
    "csv_data = csv_data[['prep_phrase_context', 'label'] + list(feature_cols)]\n",
    "\n",
    "with open('scaler.pickle', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "\n",
    "text_data, labels, features = csv_data.values[:, 0], csv_data.values[:, 1], csv_data.values[:, 2:]\n",
    "scaled_features = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'roberta-ru'\n",
    "\n",
    "if model_type == 'bert': \n",
    "    hugg_name = 'sberbank-ai/ruBert-large'\n",
    "elif model_type == 't5':\n",
    "    hugg_name = 'sberbank-ai/ruT5-large'\n",
    "elif model_type == 'roberta':\n",
    "    hugg_name = 'xlm-roberta-large'\n",
    "elif model_type == 'roberta-ru':\n",
    "    hugg_name = 'DeepPavlov/xlm-roberta-large-en-ru'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hugg_name)\n",
    "\n",
    "config = {'MAX_LEN': 512, 'tokenizer': tokenizer, 'model': model_type}\n",
    "\n",
    "dataset = Dataset(text_data, \n",
    "                labels, \n",
    "                config=config,\n",
    "                features=scaled_features\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1008073/48412968.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_label(dataset, model):\n",
    "    batch = next(iter(dataset))\n",
    "    ids = batch['ids']\n",
    "    attention_mask = batch['att_mask']\n",
    "    features = batch['features']\n",
    "    out_probs = model.forward(input_ids=ids, attention_mask=attention_mask, features=features)\n",
    "    pred_label = torch.where(out_probs > 0.5, 1, 0).item()\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label = torch.where(out_probs > 0.5, 1, 0)\n",
    "pred_label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4350.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29000 * 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4108.333333333333"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29000 * 0.85 / 3 * 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
